{
  "hash": "187fe664357da14aeb2fd078224fde5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simple Linear Regression Lecture Notes\"\ndate: 2024/01/25\n---\n\n\nBelow are the lecture notes, based on the slides, for the first Simple Linear Regression lecture. Following these notes are draft lecture notes for subsequent lectures.\n\n---\ntitle: \"Simple Linear Regression\"\nsubtitle: \"University of Florida\"\ndate: 2024-01-25\nauthor: \"Anthony Raborn, Psychometrician\"\ninstitute: \"National Association of Boards of Pharmacy\"\nformat: \n  revealjs:\n    theme: dark\n    css: styles.css\n\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(tidyverse)\n\n# https://ufl.zoom.us/j/7362215223\n```\n:::\n\n## Introduction\n\n::: columns\n::: {.column width=\"40%\"}\n### Myself\n\n-   UF Graduate, 2019\n-   2.5 years K-12, 2.5 years C&L\n-   Reproducible and automated data analysis\n:::\n\n::: {.column width=\"60%\"}\n### Approach to Instruction\n\n-   Reproducible slides and lectures\n-   Embedded practice within lectures\n-   Provide multiple solutions to problems (where possible)\n    -   focusing on reproducibility as much as reasonable!\n:::\n:::\n\n::: notes\nFirst, a little bit about myself and my approach to teaching.\n\nI graduated from the University of Florida about 5 years ago, with a Ph.D. in Research and Evaluation Methodology and a minor in Statistics. Dr. Manley and Dr. Leite were my advisors.\n\nI worked about two and a half years with Pasco County Schools as a Supervisor, Accountability, Research, and Measurement before my current two and a half years with the National Association of Boards of Pharmacy. NABP is in the certification and licensure industry, and I work on our licensing exams and related data for prospective pharmacists.\n\nMy professional focus is on reproducible and automated data analytic processes. If I'm asked to do the same task twice, I begin thinking about how to make that process automatic through programming, which also allows me to have a record to refer to for future requests. Any research I do follows a similar process, and I have a few R packages that came from work with colleagues on improving the flexibility of scripts.\n\nMy approach to teaching mirrors from this focus. I aim to provide lectures that are easily editable and reproducible. This means that students would have direct access to my lecture slides and notes, and that I can make, track, and share changes to these notes in real-time.\n\nI do this by embedding my instruction within the software I use for analysis. For example, this lecture is created within R using Quarto and will be available for everyone to access on my GitHub account.\n\nWhile I encourage students to follow a similar mindset with statistics and data analysis, I support and expect to provide support for multiple different approaches, including different software. My aim is to help everyone get to the point of creating reproducible work, if possible, and support students who are not as technically-oriented produce good work regardless of its reproducibility.\n\nFor today, though, I will be working in R exclusively.\n:::\n\n## Building upon... {.smaller}\n\n-   Mathematical notation\n    -   bar notation, i notation, sum notation, hat notation\n-   Normal distribution\n    -   symmetric, bell-shaped\n-   statistical error\n    -   random differences\n-   correlation\n    -   strength of linear relationship\n-   statistical parameter\n    -   general understanding\n\n::: notes\nThis lecture assumes that you are familiar with the following topics, as they will be touched upon but not fully explained.\n\n1.  Mathematical notation---subscripted i, squares, square roots, sum notation, bar notation, hat notation\n2.  What the normal distribution is---most importantly, that it is a symmetrical and bell-shaped distribution, meaning that values drawn from this distribution are less likely to be observed the further they are (in either direction) from the mean.\n3.  What statistical error is---most importantly, that it means random differences between expected or estimated values and observed values, not systemic differences or differences due to things like biased measurements or mis-keyed data.\n4.  What correlation is---most importantly, that it measures the strength of the linear relationship between two variables.\n5.  What statistical parameters are--just a general understanding that they provide a mathematical way to describe a statistical model and that analyses try to estimate these values\n:::\n\n## Lesson Objectives\n\n1.  Understand what simple linear regression is and when it should be used\n2.  Estimate and Interpret regression coefficients\n3.  Utilize R for fitting regression models\n\nNext time:\n\n4.  Summarize the four main assumptions of regression\n5.  Read diagnostic plots\n6.  Perform hypothesis testing on simple linear regression models\n\n::: notes\nAt the end of this lesson, you should be able to:\n\n-   Understand what simple linear regression is, when it should be used (and by converse when it shouldn't be used).\n-   Estimate the model coefficients for regression algebraically as well as interpret these coefficients.\n-   Utilize R for fitting regression models\n\nWe won't have enough time to cover these topics, but the next lecture would also add these objectives:\n\n-   Summarize the four main assumptions of regression.\n-   Read and interpret some regression diagnostic plots, identifying obvious violations of the four assumptions\n-   Perform hypothesis testing on the coefficients of a fit regression model\n:::\n\n## What is Simple Linear Regression?\n\n-   Statistical (NOT deterministic) model\n    -   explores how variation in one variable is related to (explained by) variation in a second variable\n-   Draws a \"line of best fit\" between variables\n    -   Geometric $y = mx + b$ vs statistical $y = \\beta_0 + \\beta_1 x$\n-   Uses values of one predictor (explanatory, independent) variable *x* to estimate values of one outcome (response, dependent) variable *y*\n-   For today, *x* and *y* will be continuous\n    -   (though *x* can be categorical!)\n\n::: notes\nSimple linear regression (generally referred to as \"regression\" hereout; other regression models would be directly specified) is a statistical model that explains how the variation in one variable is related to the variation in a second variable. Sometimes, the \"related to\" part is stated as \"explained by\", which is fine but keep in mind that \"explained by\" does NOT mean \"caused by\".\n\nStatistical models are contrasted with deterministic models, which are perfect predictions of y given x. A good example of this is the relationship between Fahrenheit and Celsius: knowing one gives you an exact value for the other: $F = 32 + 1.8*C$\n\nA regression draws a \"line of best fit\"---for a specific definition of \"best\"---between the two variables. This line is in the same form as the geometric line, $y = mx + b$, but in statistical terms we usually refer to this regression line as $y = \\beta_0 + \\beta_1*x$, with $\\beta_0$ and $\\beta_1$ referred to as regression parameters.\n\nAs you can see from the statistical form, the value of one variable---x---is used to predict the value of the second variable---y. The x variable is interchangeably referred to as predictor, explanatory, or independent, while the y variable is interchangeably referred to as the outcome, response, or dependent variable. The parameters provide the mathematical relationship between the two variables as a straight line.\n\nFor today, we will consider cases where both x and y are continuous variables. However, in a simple linear regression, x can also be categorical and the model will still work.\n:::\n\n------------------------------------------------------------------------\n\n### Questions it *can* answer\n\n-   statistical, linear relationship between two variables\n-   predictions of group averages\n    -   strength of differences between mathematical groups\n-   predictions of new, individual values\n    -   generally includes interpolation\n\n::: notes\nWe can use regression to investigate the statistical, linear relationship between two variables. It gives us the predicted value of group averages---that is, the difference in the average value of the outcome variable for some mathematical and possibly theoretical group based on the predictor variable. This can be observed predictor groups or---if we are careful and the variability of the predictor allows it---unobserved, interpolated predictor groups. Note that outliers or extreme values affect the quality of your interpolation, though we will not investigate that today.\n:::\n\n------------------------------------------------------------------------\n\n### Questions it *cannot* answer\n\n-   nonlinear relationships\n    -   polynomial regression\n-   effects of multiple predictor variables\n    -   multiple regression\n-   effects of one variable to many variables\n    -   multivariate regression\n-   causal relationship (by itself)\n    -   additional requirements\n-   extrapolation\n\n::: notes\nA simple linear regression can't answer questions about nonlinear relationships between variables; we can use polynomial regression in some cases, or transformations of the variables in other cases, but using original units of x and y the relationship must be linear for simple linear regression to be appropriate.\n\nWe also can't answer questions with multiple predictor variables (yet; multiple regression can), or with multiple outcome variables (multivariate regression can).\n\nRegression, by itself, CANNOT answer causal relationships (i.e., does a change in x CAUSE a specific change in average y). There are other conditions that must be satisfied before we can make causal interpretations. For now, we assume that changes are associative only.\n\nExtrapolating outside the range of your x values is also generally inappropriate. We can use regression to give us an idea of how average y values change for x values outside of the range of x values in the sample, but the further we get from the observed range the more hazardous and inappropriate the extrapolation gets. Extrapolate with caution!\n:::\n\n## Regression \"Line of Best Fit\"\n\n### Example data\n\nWe have a record of a hypothetical sample of college-aged (18-22) males, with data for their heights (in inches) and weights (in pounds).\n\nThe data-generating process for our sample is defined below.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nx <- rnorm(100, 70, 4)\ny <- 50 + 1.75*x + rnorm(100, 0, 3)\n\nheight_data <-\n  tibble(height = x\n         , weight = y\n  )\n```\n:::\n\n::: notes\nWe're going to return to this idea of a \"line of best fit\", but first let's generate some data that we know follows a simple regression model. This data will represent a sample of 100 college-aged (18 yrs old -- 22 yrs old) males, with data collected on their height (in inches) and weight (in pounds).\n\nThe R code for generating this data can be seen here.\n\nNext is a scatterplot of the resulting data. What can we tell about the association between height and weight from this plot alone?\n\n(positive correlation; positive/linear association/relationship; as height increases weight increases)\n\nLet's consider how we can draw a line that tells us the most likely weight for someone represented by our sample, given their height.\n:::\n\n------------------------------------------------------------------------\n\n::: {.cell}\n\n```{.r .cell-code}\nheight_data %>%\n  ggplot(\n    mapping = aes(x = height, y = weight)\n    ) +\n    geom_point() +\n    xlab('Height (inches)') +\n    ylab('Weight (pounds)') +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![Basic scatterplot](01-simple-linear-regression_files/figure-html/scatterplot-height-data-1.png){width=672}\n:::\n\nMale College Students Height (in inches; x-axis) and Weight (in pounds; y-axis)\n:::\n\n::: notes\nIn regression, the line of best fit is estimated with a process known as \"least squares estimation\". We take it as true, for now, that this is the best way to create a prediction line of best fit for a linear association between two variables.\n\nThis process finds an intercept and slope that minimizes squared distance between line and observations (this distance is also known as error) for ALL observations simultaneously. Our predictions based on this line are as mathematically close as possible to all the observations in our data. In a moment, we will look at an example to help us understand why SQUARED error is used over OBSERVED error.\n:::\n\n------------------------------------------------------------------------\n\n### Line of Best Fit\n\nLeast Squares Estimator:\n\n-   Minimizes squared distance between line and observations (error) for ALL observations\n\n$$\\text{L}_{min} = \\text{argmin}(\\Sigma_{i=1}^n (y_i - \\hat{y_i})^2)$$\n\n::: notes\nThe equation here defines the problem mathematically. The important part to understand is that we are taking the difference between the observed values, $y_i$, and the values predicted by the regression line, $\\hat{y_i}$, as our error. We then square the error and add the sums. We are looking for a line that produces the values $\\hat{y_i}$ that results in the smallest possible sum of squared errors in our sample.\n:::\n\n------------------------------------------------------------------------\n\n### Bad fit:\n\n::: {.cell}\n\n```{.r .cell-code}\nheight_data <-\n  height_data %>%\n  mutate(\n    # bad_prediction = 30 + 2*height \n    bad_prediction = -.9276 + 2.471429*height\n    , bad_error = weight - bad_prediction\n    , prediction = lm(weight ~ height) |> predict()\n    , error = weight - prediction\n    , sum_error_bad = sum(bad_error)\n    , sum_error_best = sum(error)\n    , SSE_bad = sum(bad_error^2)\n    , SSE_best = sum(error^2)\n    )\n\nheight_data %>%\n  ggplot(\n    aes(x = height, y = weight)\n  ) +\n  geom_point() +\n  geom_abline(intercept = -.9276, slope = 2.471429, color = 'red') +\n  geom_linerange(aes(ymax = weight, ymin = bad_prediction)) +\n  theme_bw() +\n  ylab(\"Weight (pounds)\") +\n  xlab(\"Height (inches)\") +\n  xlim(60, 80) + ylim(150, 200)\n```\n\n::: {.cell-output-display}\n![With a poorly-fitting line](01-simple-linear-regression_files/figure-html/sample-bad-fit-height-data-1.png){width=672}\n:::\n\nMale College Students Height (x) and Weight (y)\n:::\n\n::: notes\nThis next graph shows what I am calling a \"line of bad fit\". The red line looks like it provides reasonable estimates of weight for height values near the center of the distribution, but for both shorter and taller individuals it produces estimates that are noticeably further way from the prediction line. The black lines between the points and the red line are the errors. We could manually manipulate these errors (and I will show you how to later), but for now I will record these errors and use them later.\n:::\n\n------------------------------------------------------------------------\n\n### Best fit:\n\n::: {.cell}\n\n```{.r .cell-code}\nheight_data %>%\n  ggplot(\n    aes(x = height, y = weight)\n  ) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, fullrange=TRUE) +\n  geom_linerange(aes(ymax = weight, ymin = prediction)) +\n  theme_bw() +\n  ylab(\"Weight (pounds)\") +\n  xlab(\"Height (inches)\") +\n  xlim(60, 80) + ylim(150, 200)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![With a mathematically optimal line of best fit](01-simple-linear-regression_files/figure-html/sample-good-fit-height-data-1.png){width=672}\n:::\n\nMale College Students Height (x) and Weight (y)\n:::\n\n::: notes\nIt may be hard to compare right now, but this blue line is the line of best fit using least squares estimation. The errors should be noticeably smaller at the upper and lower values of height. Again, the black lines between the points and the blue line are the errors. Let's see how these two lines compare to one another.\n:::\n\n------------------------------------------------------------------------\n\n### Both lines\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_error_text <-\n  paste0(\n    \"\\nS(e) for the red line: \"\n    , height_data$sum_error_bad[1] |> round(digits = 1)\n    , \"\\nS(e) for the blue line: \"\n    , height_data$sum_error_best[1] |> round(digits = 1)\n    , \"\\nSS(e) for the red line: \"\n    , height_data$SSE_bad[1] |> round(digits = 1)\n    , \"\\nSS(e) for the blue line: \"\n    , height_data$SSE_best[1] |> round(digits = 1)\n  )\n\nheight_data %>%\n  ggplot(\n    aes(x = height, y = weight)\n  ) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_abline(intercept = -.9276, slope = 2.471429, color = 'red') +\n  geom_text(\n    x = 75\n    , y = 160\n    , label = sum_error_text) +\n  theme_bw() +\n  ylab(\"Weight (pounds)\") +\n  xlab(\"Height (inches)\") +\n  xlim(60, 80) + ylim(150, 200)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![With Best Fit in Blue and \"Bad\" Fit in Red](01-simple-linear-regression_files/figure-html/sample-both-fits-height-data-1.png){width=672}\n:::\n\nMale College Students Height (x) and Weight (y)\n:::\n\n::: notes\nAgain, the red line is the \"bad fit\" and the blue line is the \"best fit\". These lines are the same as in the previous graphs. We also see four values imposed on the graph: S(e), the sum of errors (notice that these are regular, NOT squared, errors) for both lines, and SS(e), the sum of squared errors for both lines.\n\nNotice that the sum of unsquared errors is zero for both lines, but the sum of squared errors is lower for the blue least squares estimate line. Does everyone see why SS(e) is better than S(e) for regression lines?\n\n(unsquared errors allows positive and negative errors to cancel; multiple possible solutions with unsquared errors)\n\nIs there another method of minimizing errors that you think could also work?\n\n(e.g., absolute errors)\n:::\n\n## Deriving the Least Squares Estimator as the Line of Best Fit\n\n<div>\n\n-   Population model:\n    -   $y = \\beta_0 + \\beta_1x$\n    -   $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n-   Estimation: $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\epsilon_i$\n-   Minimize the squared errors, defined as: $\\text{L} = \\Sigma_{i=1}^n \\epsilon_i^2 = \\Sigma_{i=1}^n (y_i - \\hat{y_i})^2 = \\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2$\n\n</div>\n\n::: notes\n(make sure discussion of squared errors happened)\n\nAgain, here is the statistical equation for a regression line for our observations, with the addition of the error terms and the subscript for individual observations.\n\nRemember that to show that we are moving from the population model to an estimation, we add \"hats\" to the values that are estimated. Now, our observed x values are used to estimate $\\hat{y}$ values with our estimated regression parameters, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ .\n\nWith a bit of algebra, we can redefine the Least Squares Estimator L as a function of the estimated regression line and the observed y values.\n\nAny questions so far?\n\nFrom here, we will see some of the derivation. If you are unfamiliar or out of practice with calculus, don't worry; we will only consider the final equations moving forward. If you are interested in fully understanding the math (for example, if you want to take classes with the Statistics department), you should work on understanding these steps.\n:::\n\n------------------------------------------------------------------------\n\n### Derivatives:\n\n$\\beta_0$: $$\\frac{\\delta\\text{L}}{\\delta\\beta_0} = -2\\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0$$ $$n\\hat{\\beta_0} + \\hat{\\beta_1}\\Sigma_{i=1}^n x_i = \\Sigma_{i=1}^n y_i$$\n\n$\\beta_1$: $$\\frac{\\delta\\text{L}}{\\delta\\beta_1} = -2\\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)x_i = 0$$ $$\\hat{\\beta_0}\\Sigma_{i=1}^n x_i + \\hat{\\beta_1}\\Sigma_{i=1}^n x^2_i = \\Sigma_{i=1}^n y_i x_i$$\n\n::: notes\nThis slide shows the results of taking the partial derivatives of the sum of squared errors, with respect to $\\beta_0$ and $\\beta_1$ respectively and setting the result to 0 (as we are looking for the minimum of the sum of squares). With some extra algebra, we get the following algebraic solution to the least squares estimator for simple linear regression.\n:::\n\n------------------------------------------------------------------------\n\n### Algebraic Estimates of Least Squares\n\n$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}$$\n\n$$\\hat{\\beta_1} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}$$\n\n::: notes\nUsing these equations, we now have a way to estimate the slope and intercept of a regression equation for any data set with two continuous variables (and other situations we aren't considering at the moment).\n\nNotice that the intercept, $\\hat{\\beta_0}$, depends on the slope, $\\hat{\\beta_1}$, so if we are using these equations we need to solve for $\\hat{\\beta_1}$ first, then plug that estimate into $\\hat{\\beta_0}$\n\nWe will practice using these equations in a little bit.\n:::\n\n## Review of Regression Coefficients\n\n::: incremental\n-   $\\beta_0$: intercept\n    -   Estimated ${\\bar{y}}$ when $x = 0$\n:::\n\n::: incremental\n-   $\\beta_1$: regression slope\n    -   a one-point change in $x$ results in a $\\beta_1$-point change in the average of $y$\n    -   note: results does NOT mean causes!\n    -   a units-adjusted transformation of (Pearson) correlation\n:::\n\n::: notes\n(Knowledge Check)\n\nWith the formulas to algebraically estimate the regression coefficients completed, let's review what the parameters are and what they mean.\n\n$\\beta_0$ is the intercept. It is interpreted as the expected or mean value of the outcome variable y when the predictor variable x equals zero.\n\n$\\beta_1$ is the regression coefficient or regression slope. It is interpreted as the change in expected or mean y values resulting from a one-unit change in x values (as in, increasing x by 1 in whatever units it is measured in). Note that \"results from\" doesn't mean \"caused by\" as we are not making causal relationships right now!\n\nAn interesting mathematical fact: the regression coefficient in a simple linear regression is a units-adjust transformation of the Pearson correlation; that is, for a given data set, $\\beta_1$ and the correlation are directly related!\n:::\n\n------------------------------------------------------------------------\n\n### Relationship between $\\beta_1$ and Correlation\n\n::: {.cell}\n\n```{.r .cell-code}\nheight_data %>%\n  ggplot(\n    mapping = aes(x = height, y = weight)\n    ) +\n    geom_point() +\n    xlab('Height (inches)') +\n    ylab('Weight (pounds)') +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![Basic scatterplot](01-simple-linear-regression_files/figure-html/showing-height-data-again-1.png){width=672}\n:::\n\nMale College Students Height (in inches; x-axis) and Weight (in pounds; y-axis)\n:::\n\n::: notes\nLet's briefly show how these are related.\n\nFirst, take a look at the data we originally created. How would you characterize the correlation between height and weight? If you had to estimate the correlation coefficient, what value would you use?\n\n(characterized: strong, positive; estimate: anything greater than .7 but not 1.0, definitely not negative)\n:::\n\n------------------------------------------------------------------------\n\nPearson Correlation: $$r_{(x,y)} = \\frac{\\Sigma^n_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma_{i=1}^n(x_i-\\bar{x})^2\\Sigma_{i=1}^n(y_i-\\bar{y})^2}} = \\frac{\\text{cov}(x,y)}{\\text{sd}(x)\\text{sd}(y)}$$\n\nRegression slope: $$\\hat{\\beta_1} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}=\\frac{\\text{cov}(x,y)}{\\text{var}(x)}$$\n\n::: notes\nLet's revisit the equation for the Pearson correlation. Remember that correlation is unitless and can range from negative 1 to positive 1 but cannot go beyond this range.\n\nLet's compare that equation to the algebraic equation for beta_1. It should be evident that beta_1 is in units of y. Why?\n\n(when x increased by 1 unit, y increases by beta_1 units, so beta_1 is in y units over x units. Mathematically, cov(x,y) is in x\\*y unites, and var(x) is in x\\^2 units, so we get y/x units. However, we generally interpret beta_1 in one-x-unit changes so the x units in the denominator are not important for interpretation)\n\nSince both values use the covariance between x and y in the numerator, and the difference between the denominators is the variance of x and sd of both variables; some algebra makes them equivalent (but we'll skip over the specifics).\n:::\n\n------------------------------------------------------------------------\n\n::: columns\nRelating the two:\n\n$r = \\hat{\\beta_1}*\\frac{\\sqrt{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}}{\\sqrt{\\Sigma_{i=1}^n(y_i-\\bar{y})^2}}$\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-wrap  code-fold=\"show\" code-line-numbers=\"false\"}\npearson_correlation <-\n  cor(height_data$height, height_data$weight) |>\n  round(3)\n\nregression_slope <-\n  lm(weight ~ height, data = height_data)$coefficients[2] |>\n  round(3)\n\nnumerator <-\n  (height_data$height - mean(height_data$height))^2 |>\n  sum() |> sqrt() \ndenominator <-\n  (height_data$weight - mean(height_data$weight))^2 |>\n  sum() |> sqrt()\nunits_adjustment <-\n  (numerator/denominator) |> round(3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0(\n  \"r = \", pearson_correlation, \n  \"\\nbeta_1 = \", regression_slope, \n  \"\\nunits-adjustment = \", units_adjustment,\n  \"\\nr = beta_1 * units-adjustment?: \", \n  (regression_slope*units_adjustment) |> \n    round(3)\n  ) |>\n  cat()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr = 0.909\nbeta_1 = 1.749\nunits-adjustment = 0.52\nr = beta_1 * units-adjustment?: 0.909\n```\n\n\n:::\n:::\n:::\n\n::: notes\nOn the left, we see the results from the algebraic manipulation I alluded to. By multiplying $\\beta_1$ by the units-adjustment value, sqrt(SS(x)) / sqrt(SS(y)), the two values should be the same.\n\nOn the right is some R code that performs these calculations automatically. The `cor` function should look familiar. I took a shortcut and used the `lm` function instead of calculating $\\beta_1$ myself, but we will see on the next slide that this result is functionally equivalent to the algebraic calculations on the same data. And, after calculating the units-adjustment value for the data and multiplying it with $\\beta_1$, we see that in fact the correlation and regression slope are related.\n:::\n\n## Perform Simple Regression in R\n\n### Algebraically\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_x <-\n  mean(height_data$height)\nmean_y <-\n  mean(height_data$weight)\nSS_x <-\n  sum((height_data$height - mean_x)^2)\nS_xy <-\n  sum((height_data$height - mean_x) * (height_data$weight - mean_y))\n\nbeta_1 <-\n  S_xy / SS_x\nbeta_0 <-\n  mean_y - beta_1*mean_x\n\nprint(paste0(\"Estimated Beta1: \", beta_1 |> round(digits = 2))); print(paste0(\"Estimated Beta0: \", beta_0 |> round(digits = 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated Beta1: 1.75\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimated Beta0: 49.94\"\n```\n\n\n:::\n:::\n\n::: notes\nHere is an of how to perform regression algebraically in R. This code snippet calculates the values we need for the least squares estimate of our regression coefficients using the data we simulated at the beginning of class. Note that the estimates for our coefficients are almost identical to the coefficients we used to simulate the data.\n:::\n\n------------------------------------------------------------------------\n\n### With `lm`\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_height_weight <-   \n  lm(weight ~ height, data = height_data)  \nsummary(lm_height_weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ height, data = height_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6305 -1.8413 -0.4185  1.6182  7.0385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  49.9426     5.6982   8.765 5.78e-14 ***\nheight        1.7492     0.0808  21.650  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.888 on 98 degrees of freedom\nMultiple R-squared:  0.8271,\tAdjusted R-squared:  0.8253 \nF-statistic: 468.7 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n::: notes\nHere is another look at how we can get the same information using the `lm()` function built into R. By fitting the model, saving it to the `lm_height_weight` object, then using the `summary()` function on this object, we get a printout of the regression model, including a multitude of statistics. We will look further into these statistics in another lecture, so for now look for the Coefficients and their respective Estimate values. These values are basically equivalent to the values we found algebraically and to the values we used to simulate the data.\n:::\n\n## Practice\n\n```{=html}\n<iframe width='100%' height='80%' src='https://4kve1c-anthony0raborn.shinyapps.io/simple-linear-regression/'>\n  </iframe>\n```\n::: notes\nNow let's practice what we've learned so far about regression.\n\nThis is a shiny app, built in R, that generates random data, plots the data, and lets us pick a line that we think fits the data. It also provides information about our data (e.g., means of the variables, the sum of squares for x and y individually, and the covariance between x and y). In case you don't remember how to use these values to algebraically solve for the regression coefficients, there is a hint button you can press that will show them to you. It also shows our sum of squared errors so we know how well our guess (or math!) is at minimizing these errors.\n\nLet's generate some data and see how well we do!\n\n(spend about 5 minutes)\n:::\n\n## End\n\nQuestions?\n\nLecture available at:\n\nhttps://github.com/AnthonyRaborn/introduction-to-statistics/\n\n::: {.notes}\nAny questions?\n\n(can I find a picture to add here?)\n:::\n\n\n\n## Outline for Subsequent Lectures in Simple Linear Regression\n\n### Assumptions of Simple Regression\n\n::: incremental\n1.  Linear function between x and y\n2.  Independence of errors\n3.  Normality of errors\n4.  Equality of error variances\n:::\n\n::: incremental\n-   Notice how 3 of 4 focus on errors, NOT the observed variables!\n-   These are important for the quality of our model inferences.\n:::\n\n### Diagnosing Model Assumptions\n\n#### Linear relationship\n\n##### Good:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# label: good-example-linear-relationship\n\nrequire(tidyverse)\n\nset.seed(2)\nlinear_data <-\n  tibble(\n    x = rnorm(30, mean = 100, sd = 15)\n    , y = 30 + 10*x + rnorm(30, 0, 30)\n  )\n\nlinear_data %>%\n  ggplot(\n    aes(x = x, y = y)\n  ) +\n  geom_point() +\n  theme_bw() +\n  ggtitle('Linearly-related Data')\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n##### Bad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\nnonlinear_data <-\n  tibble(\n    x = rnorm(100, mean = 0, sd = 3)\n    , y = 30 + 10*x^2 + rnorm(100, 0, 10)\n  )\n\nnonlinear_data %>%\n  ggplot(\n    aes(x = x, y = y)\n  ) +\n  geom_point() +\n  theme_bw() +\n  ggtitle('Quadratically-related Data')\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/bad-example-linear-relationship-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### Independence of Errors\n\n##### Good:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nindependent_errors_data <-\n  tibble(\n    x = rnorm(100, mean = 45, sd = 4)\n    , y = 5 -10*x + rnorm(100, 0, 6)\n    , prediction = 5 - 10*x\n    , error = y - prediction\n  )\n\nindependent_errors_data %>%\n  ggplot(\n    aes(x = x, y = error)\n  ) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_linerange(aes(ymin = error, ymax = 0)) +\n  ggtitle(\"Residuals vs x-values\", \"With error distance lines\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/good-example-independent-errors-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n##### Bad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\ndependent_errors_data <-\n  tibble(\n    x = rnorm(100, mean = 2, sd = 4)\n    , y = 5 - 10*x + rnorm(100, x, .5)\n    , prediction = 5 - 10*x\n    , error = y - prediction\n  ) \n\ndependent_errors_data %>%\n  ggplot(\n    aes(x = x, y = error)\n  ) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_linerange(aes(ymin = error, ymax = 0)) +\n  ggtitle(\"Residuals vs x-values\", \"With error distance lines\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/bad-example-independent-errors-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### Normality of Errors\n\n##### Good:\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nset.seed(6)\nnormal_errors_data <-\n  tibble(\n    x = rnorm(1000, mean = 450, sd = 4)\n    , y = 50 + 45*x + rnorm(1000, 0, 6)\n    , prediction = 50 + 45*x\n    , error = y - prediction\n  )\n\nnormal_errors_data %>%\n  ggplot(\n    aes(x = x, y = error)\n  ) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_linerange(aes(ymin = error, ymax = 0)) +\n  ggtitle(\"Residuals vs x-values\", \"With error distance lines\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/good-example-normal-errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnormal_errors_data %>%\n  ggplot(\n    aes(x = error)\n  ) +\n  geom_histogram(bins = 40, fill = 'blue') +\n  ggtitle(\"Distribution of Errors\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/good-example-normal-errors-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n##### Bad:\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nset.seed(7)\nnonnormal_errors_data <-\n  tibble(\n    x = rnorm(1000, mean = 450, sd = 4)\n    , y = 50 + 45*x + runif(1000, min = -2, max = 2)\n    , prediction = 50 + 45*x\n    , error = y - prediction\n  )\n\nnonnormal_errors_data %>%\n  ggplot(\n    aes(x = x, y = error)\n  ) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_linerange(aes(ymin = error, ymax = 0)) +\n  ggtitle(\"Residuals vs x-values\", \"With error distance lines\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/bad-example-normal-errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnonnormal_errors_data %>%\n  ggplot(\n    aes(x = error)\n  ) +\n  geom_histogram(bins = 40, fill = 'red') +\n  ggtitle(\"Distribution of Errors\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/bad-example-normal-errors-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### Equality of Errors\n\n##### Good:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8)\nequal_errors_data <-\n  tibble(\n    x = rnorm(200, mean = -350, sd = 4)\n    , y = 50 + 45*x + rnorm(200, 0, 6)\n    , prediction = 50 + 45*x\n    , error = y - prediction\n  )\n\nequal_errors_data <-\n  equal_errors_data %>%\n  mutate(\n    quartile = ntile(x, n = 4)\n  )\n\nequal_errors_data %>%\n  ggplot(\n    aes(x = x, y = error)\n  ) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_linerange(aes(ymin = error, ymax = 0)) +\n  ggtitle(\"Residuals vs x-values\", \"With error distance lines\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/good-example-equal-errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\nequal_errors_data %>%\n  ggplot(\n    aes(x = quartile, y = error, group = quartile)\n  ) +\n  geom_boxplot() +\n  ggtitle(\"Distribution of Errors\", \"Homogeneous Errors\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/good-example-equal-errors-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n##### Bad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\nnonequal_errors_data <-\n  tibble(\n    x = rnorm(200, mean = -350, sd = 4)\n    , y = 50 + 45*x + rnorm(200, (x + 350)^2, 1)\n    , prediction = 50 + 45*x\n    , error = y - prediction\n  )\n\nnonequal_errors_data <-\n  nonequal_errors_data %>%\n  mutate(\n    quartile = ntile(x, n = 4)\n  )\n\nnonequal_errors_data %>%\n  ggplot(\n    aes(x = quartile, y = error, group = quartile)\n  ) +\n  geom_boxplot() +\n  ggtitle(\"Distribution of Errors\", \"Heterogenous Errors\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/bad-example-equal-errors-1.png){width=672}\n:::\n:::\n\n\n### R Diagnostic Plots\n\n#### Model assumptions not violated\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\"}\n\n```{.r .cell-code}\nplot(lm_height_weight)\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-20-4.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n#### Model assumptions violated\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\"}\n\n```{.r .cell-code}\nnonequal_errors_lm <-\n  lm(y ~ x, data = nonequal_errors_data)\n\nplot(nonequal_errors_lm)\n```\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](01-simple-linear-regression_files/figure-html/unnamed-chunk-21-4.png){width=672}\n:::\n:::\n\n\n### Future Regression Topics\n\n::: incremental\n-   No-intercept model\n-   Polynomial regression\n-   Multiple regression\n-   Logistic regression\n-   Analysis of Variance (ANOVA)\n-   Multivariate regression\n:::\n",
    "supporting": [
      "01-simple-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}