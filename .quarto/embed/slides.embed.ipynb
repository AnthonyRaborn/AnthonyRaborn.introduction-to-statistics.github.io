{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Linear Regression\n",
        "\n",
        "University of Florida\n",
        "\n",
        "Anthony Raborn, Psychometrician (National Association of Boards of\n",
        "Pharmacy)  \n",
        "2024-01-25"
      ],
      "id": "47af9ce2-4b2d-4779-9158-f7a54ddd940e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: tidyverse"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
            "✔ dplyr     1.1.4     ✔ readr     2.1.4\n",
            "✔ forcats   1.0.0     ✔ stringr   1.5.1\n",
            "✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n",
            "✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n",
            "✔ purrr     1.0.2     \n",
            "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "✖ dplyr::filter() masks stats::filter()\n",
            "✖ dplyr::lag()    masks stats::lag()\n",
            "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
          ]
        }
      ],
      "source": [
        "#| label: setup\n",
        "require(tidyverse)"
      ],
      "id": "ac2c0c98-1a3b-4cfb-bfc9-707e971c3efa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "### Myself\n",
        "\n",
        "-   UF Graduate, 2019\n",
        "-   2.5 years K-12, 2.5 years C&L\n",
        "-   Reproducible and automated data analysis\n",
        "\n",
        "### Approach to Instruction\n",
        "\n",
        "-   Mix of theory and application\n",
        "-   Embed instruction into software\n",
        "-   Provide multiple solutions to problems (where possible)\n",
        "    -   focusing on reproducibility as much as reasonable!\n",
        "\n",
        "First, a little bit about myself and my approach to teaching.\n",
        "\n",
        "I graduated from the University of Florida about 5 years ago, with a\n",
        "Ph.D. in Research and Evaluation Methodology and a minor in Statistics.\n",
        "Dr. Manley and Dr. Leite were my advisors.\n",
        "\n",
        "I worked about two and a half years with Pasco County Schools as a\n",
        "Supervisor, Accountability, Research, and Measurement before my current\n",
        "two and a half years with the National Association of Boards of\n",
        "Pharmacy. NABP is in the certification and licensure industry, and I\n",
        "work on our licensing exams and related data for prospective\n",
        "pharmacists.\n",
        "\n",
        "My professional focus is on reproducible and automated data analytic\n",
        "processes. If I’m asked to do the same task twice, I begin thinking\n",
        "about how to make that process automatic through programming, which also\n",
        "allows me to have a record to refer to for future requests. Any research\n",
        "I do follows a similar process, and I have a few R packages that came\n",
        "from work with colleagues on improving the flexibility of scripts.\n",
        "\n",
        "My approach to teaching flows from this focus. I aim to regularly mix\n",
        "theory and application as I believe that showing the implications of the\n",
        "math with data, real or simulated, helps in understanding the “why” of\n",
        "the theory and the “how” of the application.\n",
        "\n",
        "I do this by embedding my instruction, including lectures, within the\n",
        "software I use for analysis.\n",
        "\n",
        "That doesn’t mean that “my way” is the only correct way; I support and\n",
        "expect to provide support for multiple different solutions, including\n",
        "different software. My aim is to help everyone get to the point of\n",
        "creating reproducible work, if possible.\n",
        "\n",
        "For today, though, I will be working in R exclusively. These lecture\n",
        "notes were produced in R and will be available on my GitHub account.\n",
        "\n",
        "## Building upon…\n",
        "\n",
        "-   Mathematical notation\n",
        "    -   bar notation, i notation, sum notation, hat notation\n",
        "-   Normal distribution\n",
        "    -   symmetric, bell-shaped\n",
        "-   statistical error\n",
        "    -   random differences\n",
        "-   correlation\n",
        "    -   strength of linear relationship\n",
        "-   univariate *t*-tests\n",
        "    -   testing statistical significance\n",
        "\n",
        "This lecture assumes that you are familiar with the following topics, as\n",
        "they will be touched upon but not fully explained.\n",
        "\n",
        "1.  Mathematical notation—subscripted i, squares, square roots, sum\n",
        "    notation, bar notation, hat notation\n",
        "2.  What the normal distribution is—most importantly, that it is a\n",
        "    symmetrical and bell-shaped distribution, meaning that values drawn\n",
        "    from this distribution are less likely to be observed the further\n",
        "    they are (in either direction) from the mean.\n",
        "3.  What statistical error is—most importantly, that it means random\n",
        "    differences between expected or estimated values and observed\n",
        "    values, not systemic differences or differences due to things like\n",
        "    biased measurements or miskeyed data.\n",
        "4.  What correlation is—most importantly, that it measures the strength\n",
        "    of the linear relationship between two variables.\n",
        "5.  What univariate t-tests are—which includes the very basics of null\n",
        "    hypothesis significance testing.\n",
        "\n",
        "## Lesson Objectives\n",
        "\n",
        "1.  Understand what simple linear regression is and when it should be\n",
        "    used\n",
        "2.  Estimate and Interpret regression coefficients\n",
        "3.  Summarize the four main assumptions of regression\n",
        "4.  Utilize R for fitting regression models\n",
        "5.  Read diagnostic plots\n",
        "6.  Perform hypothesis testing on simple linear regression models\n",
        "\n",
        "At the end of this lesson, you should be able to:\n",
        "\n",
        "-   Understand what simple linear regression is, when it should be used\n",
        "    (and by converse when it shouldn’t be used).\n",
        "-   Estimate the model coefficients for regression algebraically as well\n",
        "    as interpret these coefficients.\n",
        "-   Summarize the four main assumptions of regression.\n",
        "-   Utilize R for fitting regression models\n",
        "-   Read and interpret some regression diagnostic plots, identifying\n",
        "    obvious violations of the four assumptions\n",
        "-   Perform hypothesis testing on the coefficients of a fit regression\n",
        "    model\n",
        "\n",
        "## What is Simple Linear Regression?\n",
        "\n",
        "-   Statistical (NOT deterministic) model\n",
        "    -   explores how variation in one variable is related to (explained\n",
        "        by) variation in a second variable\n",
        "-   Draws a “line of best fit” between variables\n",
        "    -   Geometric $y = mx + b$ vs statistical $y = \\beta_0 + \\beta_1 x$\n",
        "-   Uses values of one predictor (explanatory, independent) variable *x*\n",
        "    to estimate values of one outcome (response, dependent) variable *y*\n",
        "-   For today, *x* and *y* will be continuous\n",
        "    -   (though *x* can be categorical!)\n",
        "\n",
        "Simple linear regression (referred to as “regression” hereout, unless\n",
        "otherwise specified) is a statistical model that explains how the\n",
        "variation in one variable is related to the variation in a second\n",
        "variable. Sometimes, the “related to” part is stated as “explained by”,\n",
        "which is fine but keep in mind that “explained by” does NOT mean “caused\n",
        "by”.\n",
        "\n",
        "A regression draws a “line of best fit”—for a specific definition of\n",
        "“best”—between the two variables. This line is in the same form as the\n",
        "geometric line, $y = mx + b$, but in statistical terms we usually refer\n",
        "to this regression line as $y = \\beta_0 + \\beta_1*x$, with $\\beta_0$ and\n",
        "$\\beta_1$ referred to as regression parameters.\n",
        "\n",
        "As you can see from the statistical form, the value of one variable—x—is\n",
        "used to predict the value of the second variable—y. The x variable is\n",
        "interchangeably referred to as predictor, explanatory, or independent,\n",
        "while the y variable is interchangeably referred to as the outcome,\n",
        "response, or dependent variable. The parameters provide the mathematical\n",
        "relationship between the two variables as a straight line.\n",
        "\n",
        "For today, we will consider cases where both x and y are continuous\n",
        "variables. However, in a simple linear regression, x can also be\n",
        "categorical and the model will still work.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Questions it *can* answer\n",
        "\n",
        "-   statistical, linear relationship between two variables\n",
        "-   predictions of group averages\n",
        "    -   strength of differences between mathematical groups\n",
        "-   predictions of new, individual values\n",
        "    -   generally includes interpolation\n",
        "\n",
        "We can use regression to investigate the statistical, linear\n",
        "relationship between two variables. It gives us the predicted value of\n",
        "group averages—that is, the difference in the average value of the\n",
        "outcome variable for some mathematical and possibly theoretical group\n",
        "based on the predictor variable. This can be observed predictor groups\n",
        "or—if we are careful and the variability of the predictor allows\n",
        "it—unobserved, interpolated predictor groups. Note that outliers or\n",
        "extreme values affect the quality of your interpolation, though we will\n",
        "not investigate that today.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Questions it *cannot* answer\n",
        "\n",
        "-   nonlinear relationships\n",
        "    -   polynomial regression\n",
        "-   effects of multiple predictor variables\n",
        "    -   multiple regression\n",
        "-   effects of one variable to many variables\n",
        "    -   multivariate regression\n",
        "-   causal relationship (by itself)\n",
        "    -   additional requirements\n",
        "-   extrapolation\n",
        "\n",
        "A simple linear regression can’t answer questions about nonlinear\n",
        "relationships between variables; we can use polynomial regression in\n",
        "some cases, or transformations of the variables in other cases, but\n",
        "using original units of x and y the relationship must be linear for\n",
        "simple linear regression to be appropriate.\n",
        "\n",
        "We also can’t answer questions with multiple predictor variables (yet;\n",
        "multiple regression can), or with multiple outcome variables\n",
        "(multivariate regression can).\n",
        "\n",
        "Regression, by itself, CANNOT answer causal relationships (i.e., does a\n",
        "change in x CAUSE a specific change in average y). There are other\n",
        "conditions that must be satisfied before we can make causal\n",
        "interpretations. For now, we assume that changes are associative only.\n",
        "\n",
        "Extrapolating outside the range of your x values is also generally\n",
        "inappropriate. We can use regression to give us an idea of how average y\n",
        "values change for x values outside of the range of x values in the\n",
        "sample, but the further we get from the observed range the more\n",
        "hazardous and inappropriate the extrapolation gets. Extrapolate with\n",
        "caution!\n",
        "\n",
        "## Regression “Line of Best Fit”\n",
        "\n",
        "### Example data\n",
        "\n",
        "We have a record of a hypothetical sample of college-aged (18-22) males,\n",
        "with data for their heights (in inches) and weights (in pounds).\n",
        "\n",
        "The data-generating process for our sample is defined below."
      ],
      "id": "b516d67d-3e4d-4ba4-9b60-f5e4223b306b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: create-height-data\n",
        "#| echo: true\n",
        "\n",
        "set.seed(1)\n",
        "x <- rnorm(100, 70, 4)\n",
        "y <- 50 + 1.75*x + rnorm(100, 0, 3)\n",
        "\n",
        "height_data <-\n",
        "  tibble(x = x\n",
        "         , y = y\n",
        "  )"
      ],
      "id": "cfb6eeee-8e0a-4953-8d87-59f3cd28d88e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’re going to return to this idea of a “line of best fit”, but first\n",
        "let’s generate some data that we know follows a simple regression model.\n",
        "This data will represent a sample of 100 college-aged (18 yrs old – 22\n",
        "yrs old) males, with data collected on their height (in inches) and\n",
        "weight (in pounds).\n",
        "\n",
        "The R code for generating this data can be seen here.\n",
        "\n",
        "Next is a scatterplot of the resulting data. What can we tell about the\n",
        "association between height and weight from this plot alone?\n",
        "\n",
        "(positive correlation; positive/linear association/relationship; as\n",
        "height increases weight increases)\n",
        "\n",
        "Let’s consider how we can draw a line that tells us the most likely\n",
        "weight for someone represented by our sample, given their height.\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "68ab5bdb-81e8-41eb-a3f7-94858189a625"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "#| label: scatterplot-height-data\n",
        "#| fig-cap: 'Male College Students Height (in inches; x-axis) and Weight (in pounds; y-axis)'\n",
        "#| fig-subcap: 'Basic scatterplot'\n",
        "\n",
        "height_data %>%\n",
        "  ggplot(\n",
        "    mapping = aes(x = x, y = y)\n",
        "    ) +\n",
        "    geom_point() +\n",
        "    xlab('Height (inches)') +\n",
        "    ylab('Weight (pounds)') +\n",
        "    theme_bw()"
      ],
      "id": "3e7ede21-bae9-4c02-afaf-a58367815632"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In regression, the line of best fit is estimated with a process known as\n",
        "“least squares estimation”. This process finds an intercept and slope\n",
        "that minimizes squared distance between line and observations (this\n",
        "distance is also known as error) for ALL observations simultaneously. In\n",
        "a moment, we will look at an example to help us understand why SQUARED\n",
        "error is used over OBSERVED error.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Line of Best Fit\n",
        "\n",
        "Least Squares Estimator:\n",
        "\n",
        "-   Minimizes squared distance between line and observations (error) for\n",
        "    ALL observations\n",
        "\n",
        "$$\\text{L}_{min} = \\text{argmin}(\\Sigma_{i=1}^n (y_i - \\hat{y_i})^2)$$\n",
        "\n",
        "The equation here defines the problem mathematically. The important part\n",
        "to understand is that we are taking the difference between the observed\n",
        "values, $y_i$, and the values predicted by the regression line,\n",
        "$\\hat{y_i}$, as our error. We then square the error and add the sums. We\n",
        "are looking for a line that produces the values $\\hat{y_i}$ that results\n",
        "in the smallest possible sum of squared errors in our sample.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Bad fit:"
      ],
      "id": "5d178092-1846-440c-9a08-2bfc60841665"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "#| label: sample-bad-fit-height-data\n",
        "#| fig-cap: 'Male College Students Height (x) and Weight (y)'\n",
        "#| fig-subcap: 'With a poorly-fitting line'\n",
        "\n",
        "height_data <-\n",
        "  height_data %>%\n",
        "  mutate(\n",
        "    # bad_prediction = 30 + 2*x \n",
        "    bad_prediction = -.9276 + 2.471429*x\n",
        "    , bad_error = y - bad_prediction\n",
        "    , prediction = lm(y ~ x) |> predict()\n",
        "    , error = y - prediction\n",
        "    , sum_error_bad = sum(bad_error)\n",
        "    , sum_error_best = sum(error)\n",
        "    , SSE_bad = sum(bad_error^2)\n",
        "    , SSE_best = sum(error^2)\n",
        "    )\n",
        "\n",
        "height_data %>%\n",
        "  ggplot(\n",
        "    aes(x = x, y = y)\n",
        "  ) +\n",
        "  geom_point() +\n",
        "  geom_abline(intercept = -.9276, slope = 2.471429, color = 'red') +\n",
        "  geom_linerange(aes(ymax = y, ymin = bad_prediction)) +\n",
        "  theme_bw() +\n",
        "  ylab(\"Weight (pounds)\") +\n",
        "  xlab(\"Height (inches)\") +\n",
        "  xlim(60, 80) + ylim(150, 200)"
      ],
      "id": "0522b2e6-8cd9-4b9f-9e5e-60d9cf157a80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This next graph shows what I am calling a “line of bad fit”. The red\n",
        "line looks like it provides reasonable estimates of weight for height\n",
        "values near the center of the distribution, but for both shorter and\n",
        "taller individuals it produces estimates that are noticeably further way\n",
        "from the prediction line. The black lines between the points and the red\n",
        "line are the errors. We could manually manipulate these errors (and I\n",
        "will show you how to later), but for now I will record these errors and\n",
        "use them later.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Best fit:"
      ],
      "id": "6d767870-7379-4bfb-9ce3-bcfba1f55f1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`geom_smooth()` using formula = 'y ~ x'"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "#| label: sample-good-fit-height-data\n",
        "#| fig-cap: 'Male College Students Height (x) and Weight (y)'\n",
        "#| fig-subcap: 'With a mathematically optimal line of best fit'\n",
        "\n",
        "height_data %>%\n",
        "  ggplot(\n",
        "    aes(x = x, y = y)\n",
        "  ) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = 'lm', se = FALSE, fullrange=TRUE) +\n",
        "  geom_linerange(aes(ymax = y, ymin = prediction)) +\n",
        "  theme_bw() +\n",
        "  ylab(\"Weight (pounds)\") +\n",
        "  xlab(\"Height (inches)\") +\n",
        "  xlim(60, 80) + ylim(150, 200)"
      ],
      "id": "5ca57f94-71e3-43e4-a1aa-a285ea861e1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It may be hard to compare right now, but this blue line is the line of\n",
        "best fit using least squares estimation. The errors should be noticeably\n",
        "smaller at the upper and lower values of height. Again, the black lines\n",
        "between the points and the blue line are the errors. Let’s see how these\n",
        "two lines compare to one another.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Both lines"
      ],
      "id": "13fdc034-9ef2-4de2-b4e5-5e0263b7596d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`geom_smooth()` using formula = 'y ~ x'"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "#| label: sample-both-fits-height-data\n",
        "#| fig-cap: 'Male College Students Height (x) and Weight (y)'\n",
        "#| fig-subcap: 'With Best Fit in Blue and \"Bad\" Fit in Red'\n",
        "sum_error_text <-\n",
        "  paste0(\n",
        "    \"\\nS(e) for the red line: \"\n",
        "    , height_data$sum_error_bad[1] |> round(digits = 1)\n",
        "    , \"\\nS(e) for the blue line: \"\n",
        "    , height_data$sum_error_best[1] |> round(digits = 1)\n",
        "    , \"\\nSS(e) for the red line: \"\n",
        "    , height_data$SSE_bad[1] |> round(digits = 1)\n",
        "    , \"\\nSS(e) for the blue line: \"\n",
        "    , height_data$SSE_best[1] |> round(digits = 1)\n",
        "  )\n",
        "\n",
        "height_data %>%\n",
        "  ggplot(\n",
        "    aes(x = x, y = y)\n",
        "  ) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = 'lm', se = FALSE) +\n",
        "  geom_abline(intercept = -.9276, slope = 2.471429, color = 'red') +\n",
        "  geom_text(\n",
        "    x = 75\n",
        "    , y = 160\n",
        "    , label = sum_error_text) +\n",
        "  theme_bw() +\n",
        "  ylab(\"Weight (pounds)\") +\n",
        "  xlab(\"Height (inches)\") +\n",
        "  xlim(60, 80) + ylim(150, 200)"
      ],
      "id": "ce2e2b1c-6a00-44f1-a837-2e84ff9286e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the red line is the “bad fit” and the blue line is the “best\n",
        "fit”. These lines are the same as in the previous graphs. We also see\n",
        "four values imposed on the graph: S(e), the sum of errors (notice that\n",
        "these are regular, NOT squared, errors) for both lines, and SS(e), the\n",
        "sum of squared errors for both lines.\n",
        "\n",
        "Notice that the sum of unsquared errors is zero for both lines, but the\n",
        "sum of squared errors is lower for the blue least squares estimate line.\n",
        "Does everyone see why SS(e) is better than S(e) for regression lines?\n",
        "\n",
        "(unsquared errors allows positive and negative errors to cancel;\n",
        "multiple possible solutions with unsquared errors)\n",
        "\n",
        "Is there another method of minimizing errors that you think could also\n",
        "work?\n",
        "\n",
        "(e.g., absolute errors)\n",
        "\n",
        "## Deriving the Least Squares Estimator as the Line of Best Fit\n",
        "\n",
        "-   Statistical equation: $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
        "-   Minimize the squared errors, defined as:\n",
        "    $\\text{L} = \\Sigma_{i=1}^n \\epsilon_i^2 = \\Sigma_{i=1}^n (y_i - \\hat{y_i})^2 = \\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2$\n",
        "\n",
        "(make sure discussion of squared errors happened)\n",
        "\n",
        "Again, here is the statistical equation for a regression line, with the\n",
        "addition of the error terms and the subscript for individual\n",
        "observations.\n",
        "\n",
        "With a bit of algebra, we can redefine the Least Squares Estimator L as\n",
        "a function of the proposed regression line and the observed y values.\n",
        "\n",
        "From here, we will see some of the derivation. If you are unfamiliar or\n",
        "out of practice with calculus, don’t worry; we will only consider the\n",
        "final equations moving forward. If you are interested in fully\n",
        "understanding the math (for example, if you want to take classes with\n",
        "the Statistics department), you should work on understanding these\n",
        "steps.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Derivatives:\n",
        "\n",
        "$\\beta_0$:\n",
        "$$\\frac{\\delta\\text{L}}{\\delta\\beta_0} = -2\\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0$$\n",
        "$$n\\hat{\\beta_0} + \\hat{\\beta_1}\\Sigma_{i=1}^n x_i = \\Sigma_{i=1}^n y_i$$\n",
        "\n",
        "$\\beta_1$:\n",
        "$$\\frac{\\delta\\text{L}}{\\delta\\beta_1} = -2\\Sigma_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)x_i = 0$$\n",
        "$$\\hat{\\beta_0}\\Sigma_{i=1}^n x_i + \\hat{\\beta_1}\\Sigma_{i=1}^n x^2_i = \\Sigma_{i=1}^n y_i x_i$$\n",
        "\n",
        "This slide shows the results of taking the partial derivatives of the\n",
        "sum of squared errors, with respect to $\\beta_0$ and $\\beta_1$\n",
        "respectively and setting the result to 0 (as we are looking for the\n",
        "minimum of the sum of squares). With some extra algebra, we get the\n",
        "following algebraic solution to the least squares estimator for simple\n",
        "linear regression.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Algebraic Estimates of Least Squares\n",
        "\n",
        "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}$$\n",
        "\n",
        "$$\\hat{\\beta_1} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}$$\n",
        "\n",
        "Using these equations, we now have a way to estimate the slope and\n",
        "intercept of a regression equation for any data set with two continuous\n",
        "variables (and other situations we aren’t considering at the moment).\n",
        "\n",
        "Notice that the intercept, $\\hat{beta_0}$, depends on the slope,\n",
        "$\\hat{\\beta_1}$, so if we are using these equations we need to solve for\n",
        "$\\hat{beta_1}$ first, then plug that estimate into $\\hat{beta_0}$\n",
        "\n",
        "We will practice using these equations in a little bit.\n",
        "\n",
        "## Review of Regression Coefficients\n",
        "\n",
        "-   $\\beta_0$: intercept\n",
        "    -   Estimated ${\\bar{y}}$ when $x = 0$\n",
        "\n",
        "-   $\\beta_1$: regression slope\n",
        "    -   a one-point change in $x$ results in a $\\beta_1$-point change in\n",
        "        the average of $y$\n",
        "    -   note: results does NOT mean causes!\n",
        "    -   a units-adjusted transformation of (Pearson) correlation\n",
        "\n",
        "With the formulas to algebraically estimate the regression coefficients\n",
        "completed, let’s review what the parameters are and what they mean.\n",
        "\n",
        "$\\beta_0$ is the intercept. It is interpreted as the expected or mean\n",
        "value of the outcome variable y when the predictor variable x equals\n",
        "zero.\n",
        "\n",
        "$\\beta_1$ is the regression coefficient or regression slope. It is\n",
        "interpreted as the change in expected or mean y values resulting from a\n",
        "one-unit change in x values (as in, increasing x by 1 in whatever units\n",
        "it is measured in). Note that “results from” doesn’t mean “caused by” as\n",
        "we are not making causal relationships right now!\n",
        "\n",
        "An interesting mathematical fact: the regression coefficient in a simple\n",
        "linear regression is a units-adjust transformation of the Pearson\n",
        "correlation; that is, for a given data set, $\\beta_1$ and the\n",
        "correlation are directly related!\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Relationship between $\\beta_1$ and Correlation"
      ],
      "id": "adcc5755-0d87-45be-a2b7-9f6392cd8bd5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "#| label: showing-height-data-again\n",
        "#| fig-cap: 'Male College Students Height (in inches; x-axis) and Weight (in pounds; y-axis)'\n",
        "#| fig-subcap: 'Basic scatterplot'\n",
        "\n",
        "height_data %>%\n",
        "  ggplot(\n",
        "    mapping = aes(x = x, y = y)\n",
        "    ) +\n",
        "    geom_point() +\n",
        "    xlab('Height (inches)') +\n",
        "    ylab('Weight (pounds)') +\n",
        "    theme_bw()"
      ],
      "id": "f3457acd-5933-4d2f-9d4f-0e09d83821d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "Pearson Correlation:\n",
        "$$r_{(x,y)} = \\frac{\\Sigma^n_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma_{i=1}^n(x_i-\\bar{x})^2\\Sigma_{i=1}^n(y_i-\\bar{y})^2}} = \\frac{\\text{cov}(x,y)}{\\text{sd}(x)\\text{sd}(y)}$$\n",
        "\n",
        "Regression slope:\n",
        "$$\\hat{\\beta_1} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}=\\frac{\\text{cov}(x,y)}{\\text{var}(x)}$$\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Relating the two:\n",
        "\n",
        "$r = \\hat{\\beta_1}*\\frac{\\sqrt{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}}{\\sqrt{\\Sigma_{i=1}^n(y_i-\\bar{y})^2}}$\n",
        "\n",
        "``` r\n",
        "#| label: showing-relationship-between-slope-and-correlation\n",
        "#| echo: true\n",
        "#| code-line-numbers: false\n",
        "#| code-overflow: wrap\n",
        "\n",
        "pearson_correlation <-\n",
        "  cor(height_data$x, height_data$y) |>\n",
        "  round(3)\n",
        "\n",
        "regression_slope <-\n",
        "  lm(y ~ x, data = height_data)$coefficients[2] |>\n",
        "  round(3)\n",
        "\n",
        "numerator <-\n",
        "  sqrt(\n",
        "    sum(\n",
        "      (height_data$x - mean(height_data$x))^2\n",
        "      )\n",
        "  ) |>\n",
        "  round(3)\n",
        "denominator <-\n",
        "  sqrt(\n",
        "    sum(\n",
        "      (height_data$y - mean(height_data$y))^2\n",
        "    )\n",
        "  ) |>\n",
        "  round(3)\n",
        "units_adjustment <-\n",
        "  (numerator/denominator) |> round(3)\n",
        "\n",
        "paste0(\n",
        "  \"r = \", pearson_correlation, \n",
        "  \"\\nbeta_1 = \", regression_slope, \n",
        "  \"\\nunits-adjustment = \", units_adjustment,\n",
        "  \"\\nr = beta_1 * units-adjustment?: \", \n",
        "  (regression_slope*units_adjustment) |> \n",
        "    round(3)\n",
        "  ) |>\n",
        "  cat()\n",
        "```\n",
        "\n",
        "    r = 0.909\n",
        "    beta_1 = 1.749\n",
        "    units-adjustment = 0.52\n",
        "    r = beta_1 * units-adjustment?: 0.909\n",
        "\n",
        "On the left, we see the results from the algebraic manipulation I\n",
        "alluded to. By multiplying $\\beta_1$ by the units-adjustment value,\n",
        "sqrt(SS(x)) / sqrt(SS(y)), the two values should be the same.\n",
        "\n",
        "On the right is some R code that performs these calculations\n",
        "automatically. The `cor` function should look familiar. I took a\n",
        "shortcut and used the `lm` function instead of calculating $\\beta_1$\n",
        "myself, but we will see shortly that the result is functionally\n",
        "equivalent. And, after calculating the units-adjustment value for the\n",
        "data and multiplying it with $\\beta_1$, we see that in fact the\n",
        "correlation and regression slope are related.\n",
        "\n",
        "## Practice"
      ],
      "id": "c1c24fc6-67c9-4469-9608-a2bf07330cdb"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<iframe width='100%' height='80%' src='https://4kve1c-anthony0raborn.shinyapps.io/simple-linear-regression/'>\n",
        "  </iframe>"
      ],
      "id": "9be6f8b9-8022-454b-9b9b-e02f78d164e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s practice what we’ve learned so far about regression.\n",
        "\n",
        "This is a shiny app, built in R, that generates random data, plots the\n",
        "data, and lets us pick a line that we think fits the data. It also\n",
        "provides information about our data (e.g., means of the variables, the\n",
        "sum of squares for x and y individually, and the covariance between x\n",
        "and y). In case you don’t remember how to use these values to\n",
        "algebraically solve for the regression coefficients, there is a hint\n",
        "button you can press that will show them to you. It also shows our sum\n",
        "of squared errors so we know how well our guess (or math!) is at\n",
        "minimizing these errors.\n",
        "\n",
        "Let’s generate some data and see how well we do!\n",
        "\n",
        "(spend about 5 minutes)\n",
        "\n",
        "## Perform Simple Regression in R\n",
        "\n",
        "### Algebraically"
      ],
      "id": "9c137594-166a-4f00-ab5a-e5cfdf305668"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Estimated Beta1: 1.75\""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Estimated Beta0: 49.94\""
          ]
        }
      ],
      "source": [
        "#| label: regression-code-algebra-sample\n",
        "#| echo: true\n",
        "\n",
        "mean_x <-\n",
        "  mean(height_data$x)\n",
        "mean_y <-\n",
        "  mean(height_data$y)\n",
        "SS_x <-\n",
        "  sum((height_data$x - mean_x)^2)\n",
        "S_xy <-\n",
        "  sum((height_data$x - mean_x) * (height_data$y - mean_y))\n",
        "\n",
        "beta_1 <-\n",
        "  S_xy / SS_x\n",
        "beta_0 <-\n",
        "  mean_y - beta_1*mean_x\n",
        "\n",
        "print(paste0(\"Estimated Beta1: \", beta_1 |> round(digits = 2)))"
      ],
      "id": "fd7aa849-f85f-4c03-8de4-1d99f63dc038"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "### With `lm`"
      ],
      "id": "f131afac-be7d-4029-a7f6-c1eddfd7fc07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Call:\n",
            "lm(formula = y ~ x, data = height_data)\n",
            "\n",
            "Residuals:\n",
            "    Min      1Q  Median      3Q     Max \n",
            "-5.6305 -1.8413 -0.4185  1.6182  7.0385 \n",
            "\n",
            "Coefficients:\n",
            "            Estimate Std. Error t value Pr(>|t|)    \n",
            "(Intercept)  49.9426     5.6982   8.765 5.78e-14 ***\n",
            "x             1.7492     0.0808  21.650  < 2e-16 ***\n",
            "---\n",
            "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Residual standard error: 2.888 on 98 degrees of freedom\n",
            "Multiple R-squared:  0.8271,    Adjusted R-squared:  0.8253 \n",
            "F-statistic: 468.7 on 1 and 98 DF,  p-value: < 2.2e-16"
          ]
        }
      ],
      "source": [
        "#| label: regression-code-lm-sample\n",
        "#| echo: true\n",
        "\n",
        "lm_height_weight <-\n",
        "  lm(y ~ x, data = height_data)\n",
        "\n",
        "summary(lm_height_weight)"
      ],
      "id": "d8188570-7f03-4e80-955a-4d0ddd548b3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End"
      ],
      "id": "6932745a-9f15-4b14-8455-2be62500e955"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}